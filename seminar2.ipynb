{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Напишите на Python программу, которая записывает произвольную html страницу, производит поиск содержащихся на ней персональных данных, анонимизирует содержимое и генерирует анонимизированный html файл для просмотра браузером.\n",
    "\n",
    "2. Протестируйте вашу программу на 10 произвольных страницах. Вычислите, сколько процентов словосочетаний, являющихся персональными данными оставляет на странице ваша программа.\n",
    "\n",
    "3. Подготовьте отчет с описанием программы , теста и демонстрацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities.engine import RecognizerResult, OperatorConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using spacy model ru_core_news_lg-3.2.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"ru\", \"model_name\": \"ru_core_news_lg\"}]\n",
    "}\n",
    "\n",
    "# Create NLP engine based on configuration\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine_with_russian = provider.create_engine()\n",
    "\n",
    "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine=nlp_engine_with_russian, \n",
    "    supported_languages=[\"ru\"]\n",
    ")\n",
    "\n",
    "\n",
    "def analyze_text(analyzer, text):\n",
    "    results_russian = analyzer.analyze(text=text,\n",
    "                                       score_threshold=0.6,\n",
    "                                       language=\"ru\")\n",
    "    return(results_russian)\n",
    "\n",
    "anonymizer_engine = AnonymizerEngine()\n",
    "def anonymize_text(anonymizer, text, analyze_text):\n",
    "    result = engine.anonymize(text=text, \n",
    "                              analyzer_results=analyze_text)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_by_category(start_url, category_name, urls_number = 10):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    driver = webdriver.Chrome('C:/Users/psmolnik/Downloads/chromedriver_win32/chromedriver.exe', options=chrome_options)\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    \n",
    "    urls = []\n",
    "    driver.get(start_url + category_name)\n",
    "    contents = driver.find_elements_by_class_name(\"list-item__content\")\n",
    "    for content in contents:\n",
    "        urls.append(content.find_element_by_css_selector('a').get_attribute('href'))\n",
    "    next_page = driver.find_element_by_class_name(\"list-more\")\n",
    "    next_page_url = start_url + next_page.get_attribute(\"data-url\")\n",
    "    urls_count = len(urls)\n",
    "\n",
    "    while(urls_count < urls_number):\n",
    "        driver.get(next_page_url)\n",
    "        contents = driver.find_elements_by_class_name(\"list-item__content\")\n",
    "        for content in contents:\n",
    "            urls.append(content.find_element_by_css_selector('a').get_attribute('href'))\n",
    "        urls_count += len(contents)\n",
    "        next_page = driver.find_element_by_class_name(\"list-items-loaded\")\n",
    "        next_page_url = start_url + next_page.get_attribute(\"data-next-url\")\n",
    "\n",
    "    driver.close()\n",
    "    return urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psmolnik\\AppData\\Local\\Temp/ipykernel_21184/1559786977.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('C:/Users/psmolnik/Downloads/chromedriver_win32/chromedriver.exe', options=chrome_options)\n",
      "C:\\Users\\psmolnik\\AppData\\Local\\Temp/ipykernel_21184/1559786977.py:12: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  contents = driver.find_elements_by_class_name(\"list-item__content\")\n",
      "C:\\Users\\psmolnik\\AppData\\Local\\Temp/ipykernel_21184/1559786977.py:15: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  next_page = driver.find_element_by_class_name(\"list-more\")\n"
     ]
    }
   ],
   "source": [
    "start_url = \"https://ria.ru/\"\n",
    "categories = [\"politics\"]\n",
    "urls = []\n",
    "for category in categories:\n",
    "    urls = get_urls_by_category(start_url, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://ria.ru/20211109/mid-1758254041.html',\n",
       " 'https://ria.ru/20211109/posol-1758250560.html',\n",
       " 'https://ria.ru/20211109/gosduma-1758230843.html',\n",
       " 'https://ria.ru/20211109/vaktsinatsiya-1758212561.html',\n",
       " 'https://ria.ru/20211109/putin-1758207360.html',\n",
       " 'https://ria.ru/20211109/vstrecha-1758206975.html',\n",
       " 'https://ria.ru/20211109/mosgorizbirkom-1758202918.html',\n",
       " 'https://ria.ru/20211109/krym-1758176325.html',\n",
       " 'https://radiosputnik.ria.ru/20211108/stratplanirovanie-1758132941.html',\n",
       " 'https://radiosputnik.ria.ru/20211108/nikaragua-1758129984.html']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourse_pwd = \"./data_seminar2/source_pages/\"\n",
    "anonymize_pwd = \"./data_seminar2/anonymized_pages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing https://ria.ru/20211109/mid-1758254041.html\n",
      "processing https://ria.ru/20211109/posol-1758250560.html\n",
      "processing https://ria.ru/20211109/gosduma-1758230843.html\n",
      "processing https://ria.ru/20211109/vaktsinatsiya-1758212561.html\n",
      "processing https://ria.ru/20211109/putin-1758207360.html\n",
      "processing https://ria.ru/20211109/vstrecha-1758206975.html\n",
      "processing https://ria.ru/20211109/mosgorizbirkom-1758202918.html\n",
      "processing https://ria.ru/20211109/krym-1758176325.html\n",
      "processing https://radiosputnik.ria.ru/20211108/stratplanirovanie-1758132941.html\n",
      "processing https://radiosputnik.ria.ru/20211108/nikaragua-1758129984.html\n",
      "not successfully https://radiosputnik.ria.ru/20211108/nikaragua-1758129984.html\n"
     ]
    }
   ],
   "source": [
    "headers = requests.utils.default_headers()\n",
    "headers.update({ 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Gecko/20100101 Safari/537.36'})\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    print(\"processing\", url)\n",
    "    try:\n",
    "        page = requests.get(url).text\n",
    "\n",
    "        soup = BeautifulSoup(page)\n",
    "\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            link.replace_with(link.text)\n",
    "\n",
    "        title = soup.find(\"meta\",  {\"name\":\"analytics:title\"})[\"content\"]\n",
    "        analyze_title = analyze_text(analyzer, title)\n",
    "        result_title = anonymize_text(anonymizer_engine, title, analyze_title)\n",
    "        locations = soup.findAll(text=title)\n",
    "        for loc in locations:\n",
    "            loc.replace_with(result_title.text)\n",
    "\n",
    "        second_title = soup.find(\"h1\", attrs={'class':'article__second-title'})\n",
    "        analyze_second_title = analyze_text(analyzer, second_title.text)\n",
    "        result_second_title = anonymize_text(anonymizer_engine, second_title.text, analyze_second_title)\n",
    "        second_title.replace_with(result_second_title.text)\n",
    "\n",
    "        text_body = soup.findAll(\"div\", attrs={'class':'article__text'})\n",
    "        for text in text_body:\n",
    "            analyze_text_body = analyze_text(analyzer, text.text)\n",
    "            result_text_body = anonymize_text(anonymizer_engine, text.text, analyze_text_body)\n",
    "            text.replace_with(result_text_body.text)   \n",
    "\n",
    "        tags = soup.find(\"meta\",  {\"name\":\"analytics:tags\"})[\"content\"].split(',')\n",
    "        for tag in tags:\n",
    "            tag = tag.strip()\n",
    "            analyze_tag = analyze_text(analyzer, tag)\n",
    "            result_tag = anonymize_text(anonymizer_engine, tag, analyze_tag)\n",
    "\n",
    "            locations = soup.findAll(text=tag)\n",
    "            for loc in locations:\n",
    "                loc.replace_with(result_tag.text)\n",
    "        \n",
    "        f = open(sourse_pwd + \"page\" + str(i) + \".html\", \"w+\", encoding='utf-8')\n",
    "        f.write(page)\n",
    "        f.close()\n",
    "        \n",
    "        f = open(anonymize_pwd + \"page\" + str(i) + \".html\", \"w+\", encoding='utf-8')\n",
    "        f.write(soup.prettify())\n",
    "        f.close()\n",
    "    except:\n",
    "        print(\"not successfully\", url)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
